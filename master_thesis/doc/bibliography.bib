@misc{WebsiteEQSNews,
  title        = {EQS Group AG},
  key          = {data_source},
  howpublished = {\url {https://www.eqs-news.com/}},
  note         = {Accessed: November 10, 2024}
}
@misc{LangChain,
 title        = {Python LangChain Website},
  key          = {software},
  howpublished = {\url {https://python.langchain.com/docs/introduction/}},
  note         = {Accessed: November 10, 2024}
}
@misc{Pydantic,
 title        = {Pydantic Website},
  key          = {software},
  howpublished = {\url {https://docs.pydantic.dev/2.9/}},
  note         = {Accessed: November 10, 2024}
}
@misc{WebsiteDPAAFX,
  title        = {dpa AFX Wirtschaftsnachrichten GmbH},
  key          = {data_source},
  howpublished = {\url{https://www.dpa-afx.de/}},
  note         = {Accessed: November 10, 2024}
}
@misc{WebsiteTraderfox,
  title        = {TraderFox GmbH},
  key          = {data_source},
  howpublished = {\url{https://mobile.traderfox.com/news/dpa-compact/}},
  note         = {Accessed: November 10, 2024}
}
@misc{CosineSimilarity,
  title        = {Cosine Similarity},
  key          = {software},
  howpublished = {\url{https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity}},
  note         = {Accessed: November 10, 2024}
}
@misc{EntropyInformationTheory,
  title        = {Wikipedia: Entropy in Information Theory},
  key          = {theory},
  howpublished = {\url{https://en.wikipedia.org/wiki/Entropy_(information_theory)}},
  note         = {Accessed: November 10, 2024}
}
@misc{FunctionWords,
  title        = {Wikipedia: Function Words},
  key          = {theory},
  howpublished = {\url{https://en.wikipedia.org/wiki/Function_word}},
  note         = {Accessed: November 10, 2024}
}
@misc{ContentWords,
  title        = {Wikipedia: Content Words},
  key          = {theory},
  howpublished = {\url{https://en.wikipedia.org/wiki/Content_word}},
  note         = {Accessed: November 10, 2024}
}
@misc{StopWords,
  title        = {Wikipedia: Stop Words},
  key          = {theory},
  howpublished = {\url{https://en.wikipedia.org/wiki/Stop_word}},
  note         = {Accessed: November 10, 2024}
}
@misc{Lemmatization,
  title        = {Wikipedia: Lemmatization},
  key          = {theory},
  howpublished = {\url{https://en.wikipedia.org/wiki/Lemmatization}},
  note         = {Accessed: November 10, 2024}
}
@misc{Stemming,
  title        = {Wikipedia: Stemming},
  key          = {theory},
  howpublished = {\url{https://en.wikipedia.org/wiki/Stemming}},
  note         = {Accessed: November 10, 2024}
}
@misc{Coreference,
  title        = {Wikipedia: Coreference},
  key          = {theory},
  howpublished = {\url{https://en.wikipedia.org/wiki/Coreference}},
  note         = {Accessed: November 10, 2024}
}
@misc{Hallucination,
  title        = {Wikipedia: Hallucination in AI},
  key          = {theory},
  howpublished = {\url{https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)}},
  note         = {Accessed: November 10, 2024}
}
@misc{CorefResIsHard,
  title={Solving hard coreference problems},
  author={Peng, Haoruo and Khashabi, Daniel and Roth, Dan},
  journal={arXiv preprint arXiv:1907.05524},
  howpublished = {\url{https://arxiv.org/abs/1907.05524}},
  year={2019}
}
@misc{CorefSurvey,
  title={A brief survey on recent advances in coreference resolution},
  author={Liu, Ruicheng and Mao, Rui and Luu, Anh Tuan and Cambria, Erik},
  journal={Artificial Intelligence Review},
  volume={56},
  number={12},
  pages={14439--14481},
  year={2023},
  howpublished = {\url{https://sentic.net/survey-on-coreference-resolution.pdf}},
  publisher={Springer}
}
@misc{ClarkManning2015,
  title={Entity-centric coreference resolution with model stacking},
  author={Clark, Kevin and Manning, Christopher D},
  booktitle={Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={1405--1415},
  howpublished = {\url{https://aclanthology.org/P15-1136.pdf}},
  year={2015}
}
@misc{ProblemsWithSparseVectors,
  title        = {Stanford NLP: CS224N Lecture Slides},
  author       = {Richard Socher},
  key          = {theory},
  howpublished = {\url{https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture3.pdf}},
  note         = {Accessed: November 10, 2024}
}
@misc{spacy,
  title        = {spaCy},
  author       = {Explosion},
  key          = {platforms},
  howpublished = {\url{https://spacy.io/}},
  note         = {Accessed: November 10, 2024}
}
@misc{StanfordNLPCoref,
  title =        {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics,
                 and Speech Recognition with Language Models},
  author =       {Daniel Jurafsky and James H. Martin},
  key = {theory},
  year = {2024},
  howpublished = {\url{https://web.stanford.edu/~jurafsky/slp3/}},
  note = {Chapter 23: Coreference Resolution and Entity Linking},
  edition =         {3rd}
}
@misc{StanfordNLPNER,
  title =        {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics,
                 and Speech Recognition with Language Models},
  author =       {Daniel Jurafsky and James H. Martin},
  key = {theory},
  year = {2024},
  howpublished = {\url{https://web.stanford.edu/~jurafsky/slp3/}},
  note = {Chapter 17: Sequence Labeling for Parts of Speech and Named Entities},
  edition =         {3rd}
}
@misc{CorefEndToEnd,
  title={End-to-end neural coreference resolution},
  author={Lee, Kenton and He, Luheng and Lewis, Mike and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1707.07045},
  howpublished = {\url{https://arxiv.org/abs/1707.07045}},
  year={2017}
}
@misc{CorefCoarseFine,
  title={Higher-order coreference resolution with coarse-to-fine inference},
  author={Lee, Kenton and He, Luheng and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1804.05392},
  howpublished = {\url{https://arxiv.org/abs/1804.05392}},
  year={2018}
}
@misc{CorefWithBert,
  title={BERT for coreference resolution: Baselines and analysis},
  author={Joshi, Mandar and Levy, Omer and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1908.09091},
  howpublished = {\url{https://arxiv.org/abs/1908.09091}},
  year={2019}
}
@misc{LSTM,
  title={Long Short-term Memory},
  author={Hochreiter, S},
  journal={Neural Computation MIT-Press},
  howpublished = {\url{https://blog.xpgreat.com/file/lstm.pdf}},
  year={1997}
}
@misc{SpanBERT,
      title={SpanBERT: Improving Pre-training by Representing and Predicting Spans},
      author={Mandar Joshi and Danqi Chen and Yinhan Liu and Daniel S. Weld and Luke Zettlemoyer and Omer Levy},
      year={2020},
      eprint={1907.10529},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={\url{https://arxiv.org/abs/1907.10529}},
}
@misc{FastCoref,
  title={F-coref: Fast, accurate and easy to use coreference resolution},
  author={Otmazgin, Shon and Cattan, Arie and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2209.04280},
  howpublished={\url{https://arxiv.org/abs/2209.04280}},
  year={2022}
}
@misc{KirstainCoref,
      title={Coreference Resolution without Span Representations},
      author={Yuval Kirstain and Ori Ram and Omer Levy},
      year={2021},
      eprint={2101.00434},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={\url{https://arxiv.org/abs/2101.00434}}
}
@misc{CorefSeq2Seq,
  title = {Coreference Resolution through a seq2seq Transition-Based System},
  author = {Bohnet, Bernd and Alberti, Chris and Collins, Michael},
  publisher = {TACL},
  howpublished={\url{https://doi.org/10.48550/arxiv.2211.12142}},
  year = {2023},
}
@misc{CorefDobrovolskii,
    title = {Word-Level Coreference Resolution},
    author = {Dobrovolskii, Vladimir},
    booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
    year = {2021},
    publisher = {Association for Computational Linguistics},
    howpublished={\url{https://aclanthology.org/2021.emnlp-main.605}},
    pages = {7670--7675},
}
@misc{BERT,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished={\url{https://arxiv.org/abs/1810.04805}}
}
@misc{CorefEvaluation,
    title = {A Controlled Reevaluation of Coreference Resolution Models},
    author = {Porada, Ian  and
      Zou, Xiyuan  and
      Cheung, Jackie Chi Kit},
    booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
    month = may,
    year = {2024},
    address = {Torino, Italia},
    publisher = {ELRA and ICCL},
    howpublished={\url{https://aclanthology.org/2024.lrec-main.23}},
    pages = {256--263},
}
@misc{AllenNLP,
  title={Allennlp: A deep semantic natural language processing platform},
  author={Gardner, Matt and Grus, Joel and Neumann, Mark and Tafjord, Oyvind and Dasigi, Pradeep and Liu, Nelson and Peters, Matthew and Schmitz, Michael and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1803.07640},
  howpublished={\url{https://arxiv.org/abs/1803.07640}},
  year={2018},
}
@misc{AllenNLPCorefModel,
  title        = {Allennlp: Coreference Model},
  howpublished = {\url{https://github.com/allenai/allennlp-models/blob/main/allennlp_models/modelcards/coref-spanbert.json}},
  note         = {Accessed: November 10, 2024}
}
@misc{AllenAI,
  title        = {AllenAI: The Allen Institute for Artificial Intelligence},
  howpublished = {\url{https://allenai.org/}},
  note         = {Accessed: November 10, 2024}
}
@misc{AllenCorefModelModification,
  title        = {Medium: How to make an effective coreference resolution model},
  howpublished = {\url{https://towardsdatascience.com/how-to-make-an-effective-coreference-resolution-model-55875d2b5f19}},
  note         = {Accessed: November 10, 2024}
}
@misc{xxCorefMinilmModel,
  title        = {Crosslingual-Coreference: minlm},
  howpublished = {\url{https://huggingface.co/microsoft/Multilingual-MiniLM-L12-H384}},
  note         = {Accessed: November 10, 2024}
}
@misc{xxCoref,
author = {David, Berenstein},
title = {{Crosslingual Coreference - a multi-lingual approach to AllenNLP CoReference Resolution along with a wrapper for spaCy.}},
version = {0.2.9},
howpublished = {\url{https://github.com/davidberenstein1957/crosslingual-coreference}},
year = {2022}
}
@misc{Coreferee,
    author = {Hudson, Richard Paul},
  title        = {Coreferee},
  howpublished = {\url{https://github.com/richardpaulhudson/coreferee}},
  note         = {Accessed: November 10, 2024}
}
@misc{word2vec1,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  journal={Advances in neural information processing systems},
  howpublished = {\url{https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf}},
  volume={26},
  year={2013}
}
@misc{word2vec2,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas},
  journal={arXiv preprint arXiv:1301.3781},
  howpublished = {\url{https://www.khoury.northeastern.edu/home/vip/teach/DMcourse/4_TF_supervised/notes_slides/1301.3781.pdf}},
  year={2013}
}
@misc{humanattention,
  title={The Role of Attention in Word Recognition: Results from OB1-Reader},
  author={Meeter, Martijn and Marzouki, Yousri and Avramiea, Arthur E and Snell, Joshua and Grainger, Jonathan},
  journal={Cognitive science},
  volume={44},
  number={7},
  pages={e12846},
  year={2020},
  publisher={Wiley Online Library},
  howpublished={\url{https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.12846}}
}
@misc{LfdTalk15,
  title={Talk 15: Transformer Applications in NLP},
  author={Gogel, Rainer},
  note         = {Frankfurt UAS, Master Program in Computer Science, Module: Learning from Data, Prof.Dr.Joerg Schaefer},
  howpublished = {\url{https://github.com/rainergo/Fileserver/blob/master/transformer_applications_in_nlp.pdf?raw=true}},
  year={2023}
}
@misc{aiayn,
title	= {Attention is All You Need},
author	= {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
note = {Google Brain, Google Research, University of Totonto},
year	= {2017},
howpublished = {\url{https://arxiv.org/pdf/1706.03762.pdf}},
}
@misc{peters2017,
  title={Semi-supervised sequence tagging with bidirectional language models},
  author={Peters, Matthew E and Ammar, Waleed and Bhagavatula, Chandra and Power, Russell},
  journal={arXiv preprint arXiv:1705.00108},
  howpublished = {\url{https://arxiv.org/abs/1705.00108}},
  year={2017}
}
@misc{elmo,
      title={Dissecting Contextual Word Embeddings: Architecture and Representation},
      author={Matthew E. Peters and Mark Neumann and Luke Zettlemoyer and Wen-tau Yih},
      year={2018},
      eprint={1808.08949},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished = {\url{https://arxiv.org/abs/1808.08949}},
}
@misc{ulmfit,
  title={Universal language model fine-tuning for text classification},
  author={Howard, Jeremy and Ruder, Sebastian},
  journal={arXiv preprint arXiv:1801.06146},
  year={2018},
  howpublished = {\url{https://arxiv.org/abs/1801.06146}},
}
@misc{transformersyoutube,
title = {Visual Guide to Transformer Neural Networks Series - Episode 2},
date = {2020},
organization = {Youtube},
author = {Hedu AI Math of Intelligence},
howpublished = {\url{https://www.youtube.com/watch?v=mMa2PmYJlCo}},
note = {Accessed: 2024-08-15}
}
@misc{WebsiteChatGPTLawyer,
  title        = {Lawyer Used ChatGPT In Court—And Cited Fake Cases. A Judge Is Considering Sanctions},
  key          = {news},
  howpublished = {\url{https://www.forbes.com/sites/mollybohannon/2023/06/08/lawyer-used-chatgpt-in-court-and-cited-fake-cases-a-judge-is-considering-sanctions/}},
  note         = {Accessed: September 7, 2024}
}
@misc{hallucinationinevitable,
      title={Hallucination is Inevitable: An Innate Limitation of Large Language Models},
      author={Ziwei Xu and Sanjay Jain and Mohan Kankanhalli},
      year={2024},
      eprint={2401.11817},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      howpublished = {\url{https://arxiv.org/abs/2401.11817}},
}
@misc{hallucinationlegal,
  title={Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools},
  author={Magesh, Varun and Surani, Faiz and Dahl, Matthew and Suzgun, Mirac and Manning, Christopher D and Ho, Daniel E},
  journal={arXiv preprint arXiv:2405.20362},
  year={2024},
  howpublished = {\url{https://arxiv.org/abs/2405.20362}},
}
@misc{spacyembeddings,
  title={Multi hash embeddings in spaCy},
  author={Miranda, Lester James and K{\'a}d{\'a}r, {\'A}kos and Boyd, Adriane and Van Landeghem, Sofie and S{\o}gaard, Anders and Honnibal, Matthew},
  journal={arXiv preprint arXiv:2212.09255},
  year={2022},
  howpublished={\url{https://arxiv.org/abs/2212.09255}}
}
@misc{gptner,
  title={GPT-NER: Named entity recognition via large language models},
  author={Wang, Shuhe and Sun, Xiaofei and Li, Xiaoya and Ouyang, Rongbin and Wu, Fei and Zhang, Tianwei and Li, Jiwei and Wang, Guoyin},
  journal={arXiv preprint arXiv:2304.10428},
  year={2023},
  howpublished={\url{https://arxiv.org/pdf/2304.10428}}
}

@misc{promptner,
  title={PromptNER: Prompting For FewShot Named Entity Recognition},
  author={Ashok, Dhananjay and Lipton, Zachary Chase},
  howpublished={\url{https://openreview.net/pdf?id=WDQ9ZzsgDL}}
}

@misc{gliner,
  title={Gliner: Generalist model for named entity recognition using bidirectional transformer},
  author={Zaratiana, Urchade and Tomeh, Nadi and Holat, Pierre and Charnois, Thierry},
  journal={arXiv preprint arXiv:2311.08526},
  year={2023},
  howpublished={\url{https://arxiv.org/pdf/2311.08526}}
}

@misc{nerbert,
  title={NER-BERT: a pre-trained model for low-resource entity tagging},
  author={Liu, Zihan and Jiang, Feijun and Hu, Yuxiang and Shi, Chen and Fung, Pascale},
  journal={arXiv preprint arXiv:2112.00405},
  year={2021},
  howpublished={\url{https://arxiv.org/pdf/2112.00405}}
}
@misc{bertbaseNER,
  title={bert-base-NER},
  author = {David S. Lim},
  key          = {model},
  howpublished = {\url{https://huggingface.co/dslim/bert-base-NER}},
  note         = {Accessed: October 24, 2024}
}
@misc{deberta,
  title={Deberta: Decoding-enhanced bert with disentangled attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2006.03654},
  year={2020},
  howpublished = {\url{https://arxiv.org/abs/2006.03654}}
}
@misc{zhangcorefseq2seq,
  title={Seq2seq is all you need for coreference resolution},
  author={Zhang, Wenzheng and Wiseman, Sam and Stratos, Karl},
  journal={arXiv preprint arXiv:2310.13774},
  year={2023},
  howpublished = {\url{https://aclanthology.org/2023.emnlp-main.704.pdf}}
}
@misc{googlet5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020},
  howpublished = {\url{https://arxiv.org/abs/1910.10683v4}}
}
@misc{iebook,
title={Foundation Models for Information Extraction},
author={Paa{\ss}, Gerhard and Giesselbach, Sven},
booktitle={Foundation Models for Natural Language Processing: Pre-trained Language Models Integrating Media},
year={2023},
publisher={Springer International Publishing},
pages={187--226},
howpublished={\url{https://link.springer.com/content/pdf/10.1007/978-3-031-23190-2.pdf}}
}
@misc{blueprints,
  title={Blueprints for text analytics using Python},
  author={Albrecht, Jens and Ramachandran, Sidharth and Winkler, Christian},
  year={2020},
  publisher={" O'Reilly Media, Inc."},
  howpublished={\url{https://github.com/blueprints-for-text-analytics-python/blueprints-text}}
}
@misc{huggingface,
  title        = {Hugging Face - Website },
  key          = {data_source},
  howpublished = {\url {https://huggingface.co/}},
  note         = {Accessed: November 10, 2024}
}
@misc{topic-cardiffnlp,
  title        = {Cardiff NLP - Pre-Trained Topic Model},
  key          = {data_source},
  howpublished = {\url {https://huggingface.co/cardiffnlp/twitter-roberta-base-dec2021-tweet-topic-multi-all}},
  note         = {Accessed: November 10, 2024}
}
@misc{topic-stefanidis,
  title        = {Dimos Stefanidis - Pre-Trained Topic Model},
  key          = {data_source},
  howpublished = {\url {https://huggingface.co/dstefa/roberta-base_topic_classification_nyt_news}},
  note         = {Accessed: November 10, 2024}
}
@misc{leaderboard-topic-models,
  title        = {Papers with Code: Topic Models - Leaderboard},
  key          = {performance},
  howpublished = {\url {https://paperswithcode.com/task/topic-models}},
  note         = {Accessed: November 10, 2024}
}
@misc{bertopic,
  title={BERTopic: Neural topic modeling with a class-based TF-IDF procedure},
  author={Grootendorst, Maarten},
  journal={arXiv preprint arXiv:2203.05794},
  year={2022},
  howpublished = {\url {https://maartengr.github.io/BERTopic/index.html}}
}
@misc{sentence-bert,
  title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author = {Reimers, Nils and Gurevych, Iryna},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  month = {11},
  year = {2019},
  publisher = {Association for Computational Linguistics},
  howpublished = {\url {https://arxiv.org/abs/1908.10084}}
}
@misc{nmf-implementation,
  title        = {NMF},
  key          = {software},
  howpublished = {\url{https://scikit-learn.org/dev/modules/generated/sklearn.decomposition.NMF.html}},
  note         = {Accessed: November 10, 2024}
}
@misc{openbb,
  title        = {OpenBB Platform},
  key          = {data},
  howpublished = {\url{https://openbb.co/products/platform}},
  note         = {Accessed: November 10, 2024}
}
@misc{neo4j,
  title        = {neo4j: Getting started},
  key          = {tools},
  howpublished = {\url{https://neo4j.com/docs/getting-started/}},
  note         = {Accessed: November 10, 2024}
}
@misc{neo4j-kgdefinition,
  title        = {neo4j: What is a Knowledge Graph},
  key          = {tools},
  howpublished = {\url{https://neo4j.com/blog/what-is-knowledge-graph/}},
  note         = {Accessed: November 10, 2024}
}
@misc{graph-definition,
  title        = {Wikipedia: Graph: Discrete Mathmatics},
  key          = {definitions},
  howpublished = {\url{https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)}},
  note         = {Accessed: November 10, 2024}
}
@misc{owl,
  title        = {Wikipedia: OWL: Web Ontology Language},
  key          = {definitions},
  howpublished = {\url{https://de.wikipedia.org/wiki/Web_Ontology_Language}},
  note         = {Accessed: November 10, 2024}
}
@misc{triples,
  title        = {Wikipedia: Triples: Subject-Predicate-Object},
  key          = {definitions},
  howpublished = {\url{https://en.wikipedia.org/wiki/Semantic_triple}},
  note         = {Accessed: November 10, 2024}
}
@misc{triple-store,
  title        = {Wikipedia: Triplestore},
  key          = {definitions},
  howpublished = {\url{https://en.wikipedia.org/wiki/Triplestore}},
  note         = {Accessed: November 10, 2024}
}
@misc{dbpedia,
  title        = {DBPedia},
  key          = {definitions},
  howpublished = {\url{https://www.dbpedia.org/}},
  note         = {Accessed: November 10, 2024}
}
@misc{wikidata,
  title        = {Wikidata},
  key          = {definitions},
  howpublished = {\url{https://www.wikidata.org/wiki/Wikidata:Main_Page}},
  note         = {Accessed: November 10, 2024}
}
@misc{turtle,
  title        = {Wikipedia: Turtle - Terse RDF Triple Language},
  key          = {definitions},
  howpublished = {\url{https://en.wikipedia.org/wiki/Turtle_(syntax)}},
  note         = {Accessed: November 10, 2024}
}
@misc{protege,
  title        = {Protege: An Open-Source Ontology Editor},
  key          = {tools},
  howpublished = {\url{https://protege.stanford.edu/}},
  note         = {Accessed: November 10, 2024}
}
@misc{rdfs,
  title        = {Wikipedia: RDFS: Resource Description Framework Schema},
  key          = {tools},
  howpublished = {\url{https://en.wikipedia.org/wiki/RDF_Schema}},
  note         = {Accessed: November 10, 2024}
}
@misc{w3c,
  title        = {W3C: World Wide Web Consortium},
  key          = {tools},
  howpublished = {\url{https://www.w3.org/}},
  note         = {Accessed: November 10, 2024}
}

@misc{rdflib,
  title        = {RDFLib: Python package for working with RDF},
  key          = {tools},
  howpublished = {\url{https://rdflib.readthedocs.io/en/stable/}},
  note         = {Accessed: November 10, 2024}
}
@misc{cypher,
  title        = {Wikipedia: Cypher Query Language},
  key          = {tools},
  howpublished = {\url{https://en.wikipedia.org/wiki/Cypher_(query_language)}},
  note         = {Accessed: November 10, 2024}
}
@misc{sql,
  title        = {Wikipedia: SQL},
  key          = {tools},
  howpublished = {\url{https://en.wikipedia.org/wiki/SQL}},
  note         = {Accessed: November 10, 2024}
}
@misc{sparql,
  title        = {SPARQL: SPARQL Protocol and RDF Query Language},
  key          = {tools},
  howpublished = {\url{https://www.w3.org/TR/sparql11-query/}},
  note         = {Accessed: November 10, 2024}
}
@misc{neo4j-apoc,
  title        = {neo4j APOC: Awesome Procedures On Cypher},
  key          = {tools},
  howpublished = {\url{https://neo4j.com/labs/apoc/}},
  note         = {Accessed: November 10, 2024}
}
@misc{wiki-entity-id,
  title        = {Wikidata Entity Identifier},
  key          = {tools},
  howpublished = {\url{https://www.wikidata.org/wiki/Wikidata:Identifiers}},
  note         = {Accessed: November 10, 2024}
}
@misc{owl-guide,
  title        = {W3C: OWL Web Ontology Language Guide},
  key          = {tools},
  howpublished = {\url{https://www.w3.org/TR/owl-guide/}},
  note         = {Accessed: November 10, 2024}
}
@misc{rag-cririque,
  author = {Leon Zucchini},
  title        = {Why Your RAG System Is Failing — and How to Fix It},
  key          = {research},
  howpublished = {\url{https://blog.curiosity.ai/%EF%B8%8F-why-your-rag-system-is-failing-and-how-to-fix-it-7fe66780a335}},
  note         = {Accessed: November 10, 2024}
}
@misc{projdigi,
  title={Projekt Digitalisierung: Information Extraction from unstructured data},
  author={Gogel, Rainer and Singh, Priya and Unkart, Christopher},
  note         = {Frankfurt UAS, Master Program in Computer Science, Projekt Digitalisierung, Prof.Dr.Martin Simon},
  howpublished = {\url{https://github.com/rainergo/UASFRA-MS-PROJDIGI}},
  year={2024}
}
@misc{llamaindex,
  title        = {LlamaIndex: LLM Framework},
  key          = {tools},
  howpublished = {\url{https://www.llamaindex.ai/}},
  note         = {Accessed: November 10, 2024}
}
@misc{dspy,
  title        = {DSPy: LLM Framework},
  key          = {tools},
  howpublished = {\url{https://dspy-docs.vercel.app/intro/}},
  note         = {Accessed: November 10, 2024}
}
@misc{graphrag,
  title={From local to global: A graph rag approach to query-focused summarization},
  author={Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley, Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and Larson, Jonathan},
  journal={arXiv preprint arXiv:2404.16130},
  year={2024},
  howpublished = {\url{https://arxiv.org/pdf/2404.16130}}
}
@misc{kaggle,
  title={Kaggle: German News Datasets},
  year={2024},
  howpublished = {\url{https://www.kaggle.com/datasets?search=news+german}}
}
@misc{pandas,
  title={pandas: Documentation},
  year={2024},
  howpublished = {\url{https://pandas.pydata.org/docs/index.html}}
}
@misc{parquet,
  title={parquet: Documentation},
  year={2024},
  howpublished = {\url{https://parquet.apache.org/}}
}

@misc{bloomberg,
  title={parquet: Documentation},
  year={2024},
  howpublished = {\url{https://www.bloomberg.com/company/}}
}
@misc{anaphora,
  title        = {Wikipedia: Anaphora},
  key          = {tools},
  howpublished = {\url{https://en.wikipedia.org/wiki/Anaphora_(linguistics)}},
  note         = {Accessed: November 10, 2024}
}
@misc{cataphora,
  title        = {Wikipedia: Cataphora},
  key          = {tools},
  howpublished = {\url{https://en.wikipedia.org/wiki/Cataphora}},
  note         = {Accessed: November 10, 2024}
}
@misc{coref-cluster,
  title        = {Gabor Melli's Research: Coreference Cluster},
  key          = {tools},
  howpublished = {\url{https://www.gabormelli.com/RKB/Coreference_Cluster}},
  note         = {Accessed: November 10, 2024}
}
@misc{antecedent,
  title        = {Wikipedia: Antecedent},
  key          = {tools},
  howpublished = {\url{https://en.wikipedia.org/wiki/Antecedent_(grammar)}},
  note         = {Accessed: November 10, 2024}
}
@misc{n-gram,
  title        = {Wikipedia: N-Gram},
  key          = {definition},
  howpublished = {\url{https://en.wikipedia.org/wiki/N-gram}},
  note         = {Accessed: November 10, 2024}
}
@misc{leaderboard-ner,
  title        = {PapersWithCode: Leaderboard NER models},
  key          = {definition},
  howpublished = {\url{https://paperswithcode.com/sota/named-entity-recognition-ner-on-bc5cdr}},
  note         = {Accessed: November 10, 2024}
}
@misc{roberta,
  title={RoBERTa: A robustly optimized BERT pretraining approach. arXiv [Preprint](2019)},
  author={Liu, Y and Ott, M and Goyal, N and Du, J and Joshi, M and Chen, D and Levy, O and Lewis, M and Zettlemoyer, L and Stoyanov, V},
  journal={arXiv preprint arXiv:1907.11692},
  year={1907},
  howpublished = {\url{https://arxiv.org/abs/1907.11692}}
}
@misc{llama3,
  title        = {Facebook: Llama3 model},
  howpublished = {\url{https://ai.meta.com/blog/meta-llama-3/}},
  note         = {Accessed: November 10, 2024}
}
@misc{ollama,
  title        = {Ollama: Running LLMs locally},
  howpublished = {\url{https://ollama.com/}},
  note         = {Accessed: November 10, 2024}
}

