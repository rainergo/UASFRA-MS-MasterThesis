{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.language import Tokenizer, TOKENIZER_INFIXES, TOKENIZER_PREFIXES, TOKENIZER_SUFFIXES\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from settings.enums import NaturalLanguage\n",
    "from utils import re_patterns as repat"
   ],
   "id": "cabb26439d27bb83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TOKENIZER_PREFIXES",
   "id": "5f51bf7298029fb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_parquet(path='../data/text_samples.parquet')\n",
    "print(df.head())"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "index = 0\n",
    "text = df.loc[index, 'pp_art_text']\n",
    "text"
   ],
   "id": "c77d20c2715c0fc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Note: Breaking up sentences\n",
    "nlp_en_sent =  English()\n",
    "nlp_en_sent.add_pipe('sentencizer')"
   ],
   "id": "1c8ba4d659ac6cd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Note: Breaking up words\n",
    "\n",
    "class TopicModel:\n",
    "    def __init__(self, language: NaturalLanguage):\n",
    "        match language:\n",
    "            case NaturalLanguage.DE:\n",
    "                self.nlp = spacy.load('de_dep_news_trf')\n",
    "            case NaturalLanguage.EN:\n",
    "                self.nlp = spacy.load('en_core_web_trf')\n",
    "        self.word_separators = self.nlp.Defaults.infixes\n",
    "\n",
    "    def word_splitter(self, text: str, \n",
    "                      return_word_separators: bool = False, \n",
    "                      custom_tokenizer: bool = True) -> list:\n",
    "        # Note: these params can be adjusted: infix, prefix, suffix\n",
    "        if custom_tokenizer:\n",
    "            def custom_tokenizer(nlp: NaturalLanguage):\n",
    "                # ToDo: Create patterns\n",
    "                prefix_re = re.compile(r\"\")\n",
    "                infix_re = re.compile(r\"[-]\")\n",
    "                suffix_re = re.compile(r\"\")\n",
    "                return Tokenizer(vocab=nlp.vocab, \n",
    "                                 # prefix_search=prefix_re.search,\n",
    "                                 infix_finditer=infix_re.finditer,\n",
    "                                 # suffix_search=suffix_re.search,\n",
    "                                 )\n",
    "            self.nlp.tokenizer = custom_tokenizer(self.nlp)\n",
    "        doc = self.nlp(text)\n",
    "        words_raw = [tok.text for tok in doc]\n",
    "        if return_word_separators:\n",
    "            words = words_raw\n",
    "        else:\n",
    "            words = [exp for exp in words_raw if exp not in string.punctuation]\n",
    "        return words\n",
    "    \n",
    "    def tfidf_vectorizer(self, \n",
    "                         text: str,\n",
    "                         min_pct_of_docs_word_must_appear_in:float = 0.00,\n",
    "                         max_pct_of_docs_word_can_appear_in:float = 1.00,\n",
    "                         ) -> pd.DataFrame:\n",
    "        # Note: min: higher number -> less common words remain in text\n",
    "        # Note: max: higher number -> more common words remain in text\n",
    "        tfidf = TfidfVectorizer(min_df=min_pct_of_docs_word_must_appear_in, max_df=max_pct_of_docs_word_can_appear_in)\n",
    "        # ToDo: The text must be ALL training text to get the features that are used for prediction\n",
    "        tfidf_vectors = tfidf.fit_transform([text])\n",
    "        df_tfidf = pd.DataFrame(tfidf_vectors.toarray(), columns=tfidf.get_feature_names_out())\n",
    "        return df_tfidf\n",
    "    \n",
    "    def similarity_matrix(self, fit_transformed_model: np.ndarray):\n",
    "        pass\n",
    "        "
   ],
   "id": "c469bb5db0826303",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tm = TopicModel(language=NaturalLanguage.DE)",
   "id": "e8d52ea0ddf05a86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "res = tm.word_splitter(text=text, return_word_separators=False, custom_tokenizer=True)\n",
    "res"
   ],
   "id": "9c2d377f3bd9bee9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "res = tm.tfidf_vectorizer(text=text)\n",
    "res"
   ],
   "id": "14f946fc3f877a50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df1 = pd.DataFrame({'Entries':['man','guy','boy','girl'],'Conflict':['Yes','Yes','Yes','No']})\n",
    "\n",
    "def funcA(d):\n",
    "    d = d + 'aaa'\n",
    "    return d\n",
    "def funcB(d):\n",
    "    d = d + 'bbb'\n",
    "    return d\n",
    "\n",
    "df1['Entries'] = df1.apply(lambda x: funcA(x['Entries']) if x['Conflict'] == 'Yes' else funcB(x['Entries']), axis=1)\n",
    "df1"
   ],
   "id": "f3032d774b86c179",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
