{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T09:16:40.092403Z",
     "start_time": "2024-10-23T09:16:37.327410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from coref import CorefResolver\n",
    "from src.settings.enums import NaturalLanguage"
   ],
   "id": "bad7e535911142ea",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T09:16:44.242008Z",
     "start_time": "2024-10-23T09:16:40.899066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nat_lang = NaturalLanguage.EN\n",
    "cr = CorefResolver(natural_language=nat_lang, coref_module='coreferee')"
   ],
   "id": "851cc0329a4275a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rainergo/.local/share/virtualenvs/UASFRA-MS-Thesis-TwKC3Gx9/lib/python3.11/site-packages/spacy_transformers/layers/hf_shim.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self._model.load_state_dict(torch.load(filelike, map_location=device))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E002] Can't find factory for 'coreferee' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, ner, beam_ner, entity_ruler, tagger, morphologizer, senter, sentencizer, textcat, spancat, spancat_singlelabel, future_entity_ruler, span_ruler, textcat_multilabel, fastcoref, en.lemmatizer",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m nat_lang \u001B[38;5;241m=\u001B[39m NaturalLanguage\u001B[38;5;241m.\u001B[39mEN\n\u001B[0;32m----> 2\u001B[0m cr \u001B[38;5;241m=\u001B[39m \u001B[43mCorefResolver\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnatural_language\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnat_lang\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcoref_module\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcoreferee\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/media/rainergo/PROJECTS/UASFRA-MS-Thesis/src/D_coref/coref.py:27\u001B[0m, in \u001B[0;36mCorefResolver.__init__\u001B[0;34m(self, natural_language, use_trf, coref_module)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresolve_extension: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_nlp()\n\u001B[0;32m---> 27\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_coref\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/media/rainergo/PROJECTS/UASFRA-MS-Thesis/src/D_coref/coref.py:54\u001B[0m, in \u001B[0;36mCorefResolver.set_coref\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresolve_extension \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresolved_text\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mcase\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcoreferee\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m---> 54\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnlp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_pipe\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcoreferee\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;66;03m# self.doc_conf = dict(fastcoref={'resolve_text': True})\u001B[39;00m\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcluster_extension \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcoref_chains\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/UASFRA-MS-Thesis-TwKC3Gx9/lib/python3.11/site-packages/spacy/language.py:791\u001B[0m, in \u001B[0;36mLanguage.add_pipe\u001B[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001B[0m\n\u001B[1;32m    787\u001B[0m     pipe_component, factory_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_pipe_from_source(\n\u001B[1;32m    788\u001B[0m         factory_name, source, name\u001B[38;5;241m=\u001B[39mname\n\u001B[1;32m    789\u001B[0m     )\n\u001B[1;32m    790\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 791\u001B[0m     pipe_component \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_pipe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfactory_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m        \u001B[49m\u001B[43mraw_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mraw_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalidate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    798\u001B[0m pipe_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_pipe_index(before, after, first, last)\n\u001B[1;32m    799\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pipe_meta[name] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_factory_meta(factory_name)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/UASFRA-MS-Thesis-TwKC3Gx9/lib/python3.11/site-packages/spacy/language.py:660\u001B[0m, in \u001B[0;36mLanguage.create_pipe\u001B[0;34m(self, factory_name, name, config, raw_config, validate)\u001B[0m\n\u001B[1;32m    652\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhas_factory(factory_name):\n\u001B[1;32m    653\u001B[0m     err \u001B[38;5;241m=\u001B[39m Errors\u001B[38;5;241m.\u001B[39mE002\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    654\u001B[0m         name\u001B[38;5;241m=\u001B[39mfactory_name,\n\u001B[1;32m    655\u001B[0m         opts\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfactory_names),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    658\u001B[0m         lang_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlang,\n\u001B[1;32m    659\u001B[0m     )\n\u001B[0;32m--> 660\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(err)\n\u001B[1;32m    661\u001B[0m pipe_meta \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_factory_meta(factory_name)\n\u001B[1;32m    662\u001B[0m \u001B[38;5;66;03m# This is unideal, but the alternative would mean you always need to\u001B[39;00m\n\u001B[1;32m    663\u001B[0m \u001B[38;5;66;03m# specify the full config settings, which is not really viable.\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: [E002] Can't find factory for 'coreferee' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, ner, beam_ner, entity_ruler, tagger, morphologizer, senter, sentencizer, textcat, spancat, spancat_singlelabel, future_entity_ruler, span_ruler, textcat_multilabel, fastcoref, en.lemmatizer"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cr.nlp.pipe_names",
   "id": "5ddaee9b960a9c97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "text = \"John went to the store. He bought some groceries. Mary was at the store as well. She also bought some groceries.\"",
   "id": "edf743a340cb393e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "doc = cr.create_doc(text)",
   "id": "83c565ad0988628d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for chain in cr.get_clusters():\n",
    "    root_name = ''\n",
    "    comp_name = ''\n",
    "    root_ent_label = ''\n",
    "    \"\"\" Unfortunately, every single coref index in a chain (span) is packed into a list, \n",
    "        thus convert it to an integer by traversing over it: \"\"\"\n",
    "    for chain_index_list in chain:\n",
    "        token_index = None\n",
    "        if len(chain_index_list) > 1:\n",
    "            for chain_index in chain_index_list:\n",
    "                if isinstance(chain_index, list):\n",
    "                    token_index = chain_index[0]\n",
    "                    \n",
    "                else:\n",
    "                    token_index = chain_index\n",
    "        else:\n",
    "            token_index = chain_index_list[0]\n",
    "        print('token_index:', token_index)\n",
    "        \"\"\" Get the token for the indexes: \"\"\"\n",
    "        \n",
    "        if len(chain_index_list) > 1:\n",
    "            for chain_index in chain_index_list:\n",
    "                if isinstance(chain_index, list):\n",
    "                    index_of_token_to_be_labeled = chain_index[0]\n",
    "                else:\n",
    "                    index_of_token_to_be_labeled = chain_index\n",
    "        else:\n",
    "            index_of_token_to_be_labeled = chain_index_list[0]\n",
    "        print('index_of_token_to_be_labeled:', index_of_token_to_be_labeled)"
   ],
   "id": "dcdede51230fcb32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3c1292afab625274",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
